Subject: Use Ceph cluster stats to report disk info on RBD
 Local disk statistics on compute nodes are irrelevant when ephemeral
 disks are stored in RBD. With RBD, local disk space is not consumed when
 instances are started on a compute node, yet it is possible for
 scheduler to refuse to schedule an instance when combined disk usage of
 instances already running on the node exceeds total disk capacity
 reported by the hypervisor driver.
Origin: upstream, https://github.com/angdraug/nova/commits/rbd-ephemeral-clone-stable-icehouse
From a59b17649121ffda64d61662040c94ffaddae609 Mon Sep 17 00:00:00 2001
Author: Dmitry Borodaenko <dborodaenko@mirantis.com>
Date: Mon, 23 Jun 2014 16:12:18 -0700
Change-Id: I9718c727db205b6f2191f8435583391584e96e6e
Bug-Ubuntu: https://launchpad.net/bugs/1332660
Signed-off-by: Dmitry Borodaenko <dborodaenko@mirantis.com>

diff --git a/nova/virt/libvirt/driver.py b/nova/virt/libvirt/driver.py
index 0f972db..f5628d3 100644
--- a/nova/virt/libvirt/driver.py
+++ b/nova/virt/libvirt/driver.py
@@ -1080,12 +1080,15 @@ class LibvirtDriver(driver.ComputeDriver):
             if CONF.libvirt.images_type == 'rbd':
                 self._cleanup_rbd(instance)
 
-    def _cleanup_rbd(self, instance):
-        driver = rbd.RBDDriver(
+    @staticmethod
+    def _get_rbd_driver():
+        return rbd.RBDDriver(
                 pool=CONF.libvirt.images_rbd_pool,
                 ceph_conf=CONF.libvirt.images_rbd_ceph_conf,
                 rbd_user=CONF.libvirt.rbd_user)
-        driver.cleanup_volumes(instance)
+
+    def _cleanup_rbd(self, instance):
+        LibvirtDriver._get_rbd_driver().cleanup_volumes(instance)
 
     def _cleanup_lvm(self, instance):
         """Delete all LVM disks for given instance object."""
@@ -3837,6 +3840,8 @@ class LibvirtDriver(driver.ComputeDriver):
         if CONF.libvirt.images_type == 'lvm':
             info = libvirt_utils.get_volume_group_info(
                                  CONF.libvirt.images_volume_group)
+        elif CONF.libvirt.images_type == 'rbd':
+            info = LibvirtDriver._get_rbd_driver().get_pool_info()
         else:
             info = libvirt_utils.get_fs_info(CONF.instances_path)
 
diff --git a/nova/virt/libvirt/rbd.py b/nova/virt/libvirt/rbd.py
index 4658199..fce4b3d 100644
--- a/nova/virt/libvirt/rbd.py
+++ b/nova/virt/libvirt/rbd.py
@@ -28,6 +28,7 @@ from nova.openstack.common import excutils
 from nova.openstack.common.gettextutils import _
 from nova.openstack.common import jsonutils
 from nova.openstack.common import log as logging
+from nova.openstack.common import units
 from nova import utils
 
 LOG = logging.getLogger(__name__)
@@ -264,3 +265,10 @@ class RBDDriver(object):
                     LOG.warn(_('rbd remove %(volume)s in pool %(pool)s '
                                'failed'),
                              {'volume': volume, 'pool': self.pool})
+
+    def get_pool_info(self):
+        with RADOSClient(self) as client:
+            stats = client.cluster.get_cluster_stats()
+            return {'total': stats['kb'] * units.Ki,
+                    'free':  stats['kb_avail'] * units.Ki,
+                    'used':  stats['kb_used'] * units.Ki}
-- 
2.1.1

