Description: Backport the fixes needed for Ceph support on Havana
 This patch is an agregate of all the 9 backport patches made by Intank, and
 available from github at:
  https://github.com/jdurgin/nova/commits/havana-ephemeral-rbd
 .
 Individual patches:
  https://github.com/jdurgin/nova/commit/4015328d404cff8a1b9cec5a603832f7173e4ec2
  https://github.com/jdurgin/nova/commit/44e420501aae7e215cfe7db62343afd28dff0b09
  https://github.com/jdurgin/nova/commit/7b82cf68686d0aac7675324f4756cc0baebed04a
  https://github.com/jdurgin/nova/commit/dd49223e44c59946a0c2e7430c75e89107c0c07c
  https://github.com/jdurgin/nova/commit/bf04b9dc868118ceb86d3c3e83fcda710c6ad32d
  https://github.com/jdurgin/nova/commit/c429d229a54c9d4b71092dcdf2a7d8b84abbf029
  https://github.com/jdurgin/nova/commit/dcc2116f81eeffcf578b81daad443886e70d6711
  https://github.com/jdurgin/nova/commit/750f4032b68dbc5fc28fa9d397e1b42d51bc7fa5
  https://github.com/jdurgin/nova/commit/8e4594123b65ddf47e682876373bca6171f4a6f5
 .
 All patches applied nearly cleanly (with offsets) but the 6th patch of the
 above serries.
Author: Thomas Goirand <zigo@debian.org>
Origin: upstream, https://github.com/jdurgin/nova/commits/havana-ephemeral-rbd
Reviewed-By: leseb
Last-Update: 2014-02-14

--- nova-2013.2.2.orig/nova/tests/compute/test_compute.py
+++ nova-2013.2.2/nova/tests/compute/test_compute.py
@@ -4424,7 +4424,7 @@ class ComputeTestCase(BaseTestCase):
         fake_notifier.NOTIFICATIONS = []
         # start test
         self.mox.ReplayAll()
-        migrate_data = {'is_shared_storage': False}
+        migrate_data = {'is_shared_instance_path': False}
         ret = self.compute.pre_live_migration(c, instance=instance,
                                               block_migration=False,
                                               migrate_data=migrate_data)
@@ -4486,7 +4486,7 @@ class ComputeTestCase(BaseTestCase):
         self.compute.compute_rpcapi.remove_volume_connection(
                 c, updated_instance, 'vol2-id', dest_host)
         self.compute.compute_rpcapi.rollback_live_migration_at_destination(
-                c, updated_instance, dest_host)
+                c, updated_instance, dest_host, destroy_disks=True)
 
         # start test
         self.mox.ReplayAll()
@@ -4506,7 +4506,7 @@ class ComputeTestCase(BaseTestCase):
 
         instance = jsonutils.to_primitive(instance_ref)
 
-        migrate_data = {'is_shared_storage': False}
+        migrate_data = {'is_shared_instance_path': False}
 
         self.mox.StubOutWithMock(self.compute.compute_rpcapi,
                                  'pre_live_migration')
@@ -4586,7 +4586,7 @@ class ComputeTestCase(BaseTestCase):
 
         # start test
         self.mox.ReplayAll()
-        migrate_data = {'is_shared_storage': False}
+        migrate_data = {'is_shared_instance_path': False}
         self.compute._post_live_migration(c, inst_ref, dest,
                                           migrate_data=migrate_data)
         self.assertTrue('destroyed' in result)
--- nova-2013.2.2.orig/nova/tests/compute/test_rpcapi.py
+++ nova-2013.2.2/nova/tests/compute/test_rpcapi.py
@@ -340,7 +340,8 @@ class ComputeRpcAPITestCase(test.TestCas
 
     def test_rollback_live_migration_at_destination(self):
         self._test_compute_api('rollback_live_migration_at_destination',
-                'cast', instance=self.fake_instance, host='host')
+                'cast', instance=self.fake_instance, host='host',
+                destroy_disks=True)
 
     def test_run_instance(self):
         self._test_compute_api('run_instance', 'cast',
--- nova-2013.2.2.orig/nova/tests/virt/libvirt/test_imagebackend.py
+++ nova-2013.2.2/nova/tests/virt/libvirt/test_imagebackend.py
@@ -20,6 +20,8 @@ import os
 import fixtures
 from oslo.config import cfg
 
+from inspect import getargspec
+
 from nova import exception
 from nova.openstack.common import uuidutils
 from nova import test
@@ -293,12 +295,12 @@ class Qcow2TestCase(_ImageTestCase, test
     def test_create_image_too_small(self):
         fn = self.prepare_mocks()
         self.mox.StubOutWithMock(os.path, 'exists')
-        self.mox.StubOutWithMock(imagebackend.disk, 'get_disk_size')
+        self.mox.StubOutWithMock(imagebackend.Qcow2, 'get_disk_size')
         if self.OLD_STYLE_INSTANCE_PATH:
             os.path.exists(self.OLD_STYLE_INSTANCE_PATH).AndReturn(False)
         os.path.exists(self.TEMPLATE_PATH).AndReturn(True)
-        imagebackend.disk.get_disk_size(self.TEMPLATE_PATH
-                                       ).AndReturn(self.SIZE)
+        imagebackend.Qcow2.get_disk_size(self.TEMPLATE_PATH
+                                         ).AndReturn(self.SIZE)
         self.mox.ReplayAll()
 
         image = self.image_class(self.INSTANCE, self.NAME)
@@ -521,10 +523,12 @@ class RbdTestCase(_ImageTestCase, test.N
         self.libvirt_utils = imagebackend.libvirt_utils
         self.utils = imagebackend.utils
         self.rbd = self.mox.CreateMockAnything()
+        self.rados = self.mox.CreateMockAnything()
 
     def prepare_mocks(self):
         fn = self.mox.CreateMockAnything()
         self.mox.StubOutWithMock(imagebackend, 'rbd')
+        self.mox.StubOutWithMock(imagebackend, 'rados')
         return fn
 
     def test_cache(self):
@@ -618,11 +622,15 @@ class RbdTestCase(_ImageTestCase, test.N
         fake_processutils.fake_execute_clear_log()
         fake_processutils.stub_out_processutils_execute(self.stubs)
         self.mox.StubOutWithMock(imagebackend, 'rbd')
+        self.mox.StubOutWithMock(imagebackend, 'rados')
         image = self.image_class(self.INSTANCE, self.NAME)
 
         def fake_fetch(target, *args, **kwargs):
             return
 
+        def fake_resize(rbd_name, size):
+            return
+
         self.stubs.Set(os.path, 'exists', lambda _: True)
         self.stubs.Set(image, 'check_image_exists', lambda: True)
 
@@ -630,6 +638,132 @@ class RbdTestCase(_ImageTestCase, test.N
 
         self.assertEqual(fake_processutils.fake_execute_get_log(), [])
 
+    def test_direct_fetch(self):
+        self.prepare_mocks()
+        self.rbd.RBD_FEATURE_LAYERING = 1
+
+        image = self.image_class(self.INSTANCE, self.NAME)
+        self.mox.StubOutWithMock(image, '_clone')
+        image._clone('b', 'c', 'd')
+        self.mox.ReplayAll()
+
+        def fake_parse_location(url):
+            return 'a', 'b', 'c', 'd'
+
+        self.stubs.Set(image, 'check_image_exists', lambda: False)
+        self.stubs.Set(image, '_is_cloneable', lambda x: True)
+        self.stubs.Set(image, '_parse_location', fake_parse_location)
+
+        image.direct_fetch('image_id',
+                           {'disk_format': 'raw'},
+                           [{'url': 'rbd://a/b/c/d'}])
+
+        self.mox.VerifyAll()
+
+    def test_direct_fetch_fail(self):
+        image = self.image_class(self.INSTANCE, self.NAME)
+        self.stubs.Set(image, '_is_cloneable', lambda: True)
+        self.assertRaises(exception.ImageUnacceptable,
+                          image.direct_fetch, 'image_id',
+                          {'disk_format': 'raw'},
+                          [{'url': 'rbd://a/b/c/d'}])
+        self.rbd.RBD_FEATURE_LAYERING = 1
+        self.assertRaises(exception.ImageUnacceptable,
+                          image.direct_fetch, 'image_id',
+                          {},
+                          [{'url': 'rbd://a/b/c/d'}])
+        self.assertRaises(exception.ImageUnacceptable,
+                          image.direct_fetch, 'image_id',
+                          {'disk_format': 'raw'},
+                          [])
+        self.stubs.Set(image, '_is_cloneable', lambda: False)
+        self.assertRaises(exception.ImageUnacceptable,
+                          image.direct_fetch, 'image_id',
+                          {'disk_format': 'raw'},
+                          [{'url': 'rbd://a/b/c/d'}])
+
+    def test_good_locations(self):
+        image = self.image_class(self.INSTANCE, self.NAME)
+        locations = ['rbd://fsid/pool/image/snap',
+                     'rbd://%2F/%2F/%2F/%2F', ]
+        map(image._parse_location, locations)
+
+    def test_bad_locations(self):
+        image = self.image_class(self.INSTANCE, self.NAME)
+        locations = ['rbd://image',
+                     'http://path/to/somewhere/else',
+                     'rbd://image/extra',
+                     'rbd://image/',
+                     'rbd://fsid/pool/image/',
+                     'rbd://fsid/pool/image/snap/',
+                     'rbd://///', ]
+        for loc in locations:
+            self.assertRaises(exception.ImageUnacceptable,
+                              image._parse_location,
+                              loc)
+            self.assertFalse(image._is_cloneable(loc))
+
+    def test_cloneable(self):
+        image = self.image_class(self.INSTANCE, self.NAME)
+        self.stubs.Set(image, '_get_fsid', lambda: 'abc')
+        location = u'rbd://abc/pool/image/snap'
+        mock_proxy = self.mox.CreateMockAnything()
+        self.mox.StubOutWithMock(imagebackend, 'RBDVolumeProxy')
+
+        imagebackend.RBDVolumeProxy(image, 'image',
+                                    pool='pool',
+                                    snapshot='snap',
+                                    read_only=True).AndReturn(mock_proxy)
+        mock_proxy.__enter__().AndReturn(mock_proxy)
+        mock_proxy.__exit__(None, None, None).AndReturn(None)
+
+        self.mox.ReplayAll()
+
+        self.assertTrue(image._is_cloneable(location))
+
+    def test_uncloneable_different_fsid(self):
+        image = self.image_class(self.INSTANCE, self.NAME)
+        self.stubs.Set(image, '_get_fsid', lambda: 'abc')
+        location = 'rbd://def/pool/image/snap'
+        self.assertFalse(image._is_cloneable(location))
+
+    def test_uncloneable_unreadable(self):
+        self.prepare_mocks()
+        image = self.image_class(self.INSTANCE, self.NAME, rbd=self.rbd)
+        self.stubs.Set(image, '_get_fsid', lambda: 'abc')
+        location = 'rbd://abc/pool/image/snap'
+        self.stubs.Set(self.rbd, 'Error', test.TestingException)
+        self.mox.StubOutWithMock(imagebackend, 'RBDVolumeProxy')
+
+        imagebackend.RBDVolumeProxy(image, 'image',
+                                    pool='pool',
+                                    snapshot='snap',
+                                    read_only=True).AndRaise(
+            test.TestingException)
+
+        self.mox.ReplayAll()
+
+        self.assertFalse(image._is_cloneable(location))
+
+    def test_parent_compatible(self):
+        self.assertEqual(getargspec(imagebackend.Image.libvirt_info),
+             getargspec(self.image_class.libvirt_info))
+
+    def test_image_path(self):
+
+        conf = "FakeConf"
+        pool = "FakePool"
+        user = "FakeUser"
+
+        self.flags(libvirt_images_rbd_pool=pool)
+        self.flags(libvirt_images_rbd_ceph_conf=conf)
+        self.flags(rbd_user=user)
+        image = self.image_class(self.INSTANCE, self.NAME)
+        rbd_path = "rbd:%s/%s:id=%s:conf=%s" % (pool, image.rbd_name,
+                                                user, conf)
+
+        self.assertEqual(image.path, rbd_path)
+
 
 class BackendTestCase(test.NoDBTestCase):
     INSTANCE = {'name': 'fake-instance',
--- nova-2013.2.2.orig/nova/tests/virt/libvirt/fake_libvirt_utils.py
+++ nova-2013.2.2/nova/tests/virt/libvirt/fake_libvirt_utils.py
@@ -211,10 +211,16 @@ def pick_disk_driver_name(hypervisor_ver
 
 
 def list_rbd_volumes(pool):
-    fake_volumes = ['fakeinstancename.local', 'fakeinstancename.swap',
-                    'fakeinstancename', 'wronginstancename']
+    fake_volumes = ['875a8070-d0b9-4949-8b31-104d125c9a64.local',
+                    '875a8070-d0b9-4949-8b31-104d125c9a64.swap',
+                    '875a8070-d0b9-4949-8b31-104d125c9a64',
+                    'wrong875a8070-d0b9-4949-8b31-104d125c9a64']
     return fake_volumes
 
 
 def remove_rbd_volumes(pool, *names):
     pass
+
+
+def ascii_str(s):
+    return libvirt_utils.ascii_str(s)
--- nova-2013.2.2.orig/nova/tests/virt/libvirt/test_libvirt.py
+++ nova-2013.2.2/nova/tests/virt/libvirt/test_libvirt.py
@@ -2893,7 +2893,7 @@ class LibvirtConnTestCase(test.TestCase)
         ret = conn.check_can_live_migrate_source(self.context, instance_ref,
                                                  dest_check_data)
         self.assertTrue(type(ret) == dict)
-        self.assertTrue('is_shared_storage' in ret)
+        self.assertTrue('is_shared_instance_path' in ret)
 
     def test_check_can_live_migrate_source_vol_backed_w_disk_raises(self):
         instance_ref = db.instance_create(self.context, self.test_instance)
@@ -3185,7 +3185,7 @@ class LibvirtConnTestCase(test.TestCase)
             self.mox.StubOutWithMock(conn, 'plug_vifs')
             conn.plug_vifs(mox.IsA(inst_ref), nw_info)
             self.mox.ReplayAll()
-            migrate_data = {'is_shared_storage': False,
+            migrate_data = {'is_shared_instance_path': False,
                             'is_volume_backed': True,
                             'block_migration': False,
                             'instance_relative_path': inst_ref['name']
@@ -4356,8 +4356,10 @@ class LibvirtConnTestCase(test.TestCase)
         def fake_get_info(instance_name):
             return {'state': power_state.SHUTDOWN, 'id': -1}
 
-        fake_volumes = ['fakeinstancename.local', 'fakeinstancename.swap',
-                        'fakeinstancename', 'wronginstancename']
+        fake_volumes = ['875a8070-d0b9-4949-8b31-104d125c9a64.local',
+                        '875a8070-d0b9-4949-8b31-104d125c9a64.swap',
+                        '875a8070-d0b9-4949-8b31-104d125c9a64',
+                        'wrong875a8070-d0b9-4949-8b31-104d125c9a64']
         fake_pool = 'fake_pool'
         fake_instance = {'name': 'fakeinstancename', 'id': 'instanceid',
                          'uuid': '875a8070-d0b9-4949-8b31-104d125c9a64'}
--- nova-2013.2.2.orig/nova/image/glance.py
+++ nova-2013.2.2/nova/image/glance.py
@@ -295,7 +295,7 @@ class GlanceImageService(object):
         base_image_meta = self._translate_from_glance(image)
         return base_image_meta
 
-    def _get_locations(self, context, image_id):
+    def get_locations(self, context, image_id):
         """Returns the direct url representing the backend storage location,
         or None if this attribute is not shown by Glance.
         """
@@ -327,7 +327,7 @@ class GlanceImageService(object):
     def download(self, context, image_id, data=None, dst_path=None):
         """Calls out to Glance for data and writes data."""
         if CONF.allowed_direct_url_schemes and dst_path is not None:
-            locations = self._get_locations(context, image_id)
+            locations = self.get_locations(context, image_id)
             for entry in locations:
                 loc_url = entry['url']
                 loc_meta = entry['metadata']
--- nova-2013.2.2.orig/nova/compute/rpcapi.py
+++ nova-2013.2.2/nova/compute/rpcapi.py
@@ -678,12 +678,13 @@ class ComputeAPI(rpcclient.RpcProxy):
                    instance=instance, migration=migration,
                    reservations=reservations)
 
-    def rollback_live_migration_at_destination(self, ctxt, instance, host):
+    def rollback_live_migration_at_destination(self, ctxt, instance, host,
+                                               destroy_disks=True):
         instance_p = jsonutils.to_primitive(instance)
         cctxt = self.client.prepare(server=host,
                 version=_get_version('2.0'))
         cctxt.cast(ctxt, 'rollback_live_migration_at_destination',
-                   instance=instance_p)
+                   instance=instance_p, destroy_disks=destroy_disks)
 
     def run_instance(self, ctxt, instance, host, request_spec,
                      filter_properties, requested_networks,
--- nova-2013.2.2.orig/nova/compute/manager.py
+++ nova-2013.2.2/nova/compute/manager.py
@@ -4143,11 +4143,18 @@ class ComputeManager(manager.SchedulerDe
         # No instance booting at source host, but instance dir
         # must be deleted for preparing next block migration
         # must be deleted for preparing next live migration w/o shared storage
-        is_shared_storage = True
+        is_shared_block_storage = True
+        is_shared_instance_path = True
         if migrate_data:
-            is_shared_storage = migrate_data.get('is_shared_storage', True)
-        if block_migration or not is_shared_storage:
-            self.driver.destroy(instance_ref, network_info)
+            is_shared_block_storage = migrate_data.get(
+                    'is_shared_block_storage', True)
+            is_shared_instance_path = migrate_data.get(
+                    'is_shared_instance_path', True)
+
+        if block_migration or not is_shared_instance_path:
+            self.driver.destroy(instance_ref, network_info,
+                    destroy_disks=not is_shared_block_storage,
+                    migrate_data=migrate_data)
         else:
             # self.driver.destroy() usually performs  vif unplugging
             # but we must do it explicitly here when block_migration
@@ -4270,20 +4277,24 @@ class ComputeManager(manager.SchedulerDe
         # Also Volume backed live migration w/o shared storage needs to delete
         # newly created instance-xxx dir on the destination as a part of its
         # rollback process
-        is_volume_backed = False
-        is_shared_storage = True
+        is_shared_block_storage = False
+        is_shared_instance_path = False
         if migrate_data:
-            is_volume_backed = migrate_data.get('is_volume_backed', False)
-            is_shared_storage = migrate_data.get('is_shared_storage', True)
-        if block_migration or (is_volume_backed and not is_shared_storage):
+            is_shared_block_storage = migrate_data.get(
+                    'is_shared_block_storage', True)
+            is_shared_instance_path = migrate_data.get(
+                    'is_shared_instance_path', True)
+        if block_migration or (
+                is_shared_block_storage and not is_shared_instance_path):
             self.compute_rpcapi.rollback_live_migration_at_destination(context,
-                    instance, dest)
+                    instance, dest, destroy_disks=not is_shared_block_storage)
 
         self._notify_about_instance_usage(context, instance,
                                           "live_migration._rollback.end")
 
     @wrap_exception()
-    def rollback_live_migration_at_destination(self, context, instance):
+    def rollback_live_migration_at_destination(self, context, instance,
+                                               destroy_disks=True):
         """Cleaning up image directory that is created pre_live_migration.
 
         :param context: security context
@@ -4302,7 +4313,8 @@ class ComputeManager(manager.SchedulerDe
         #             from remote volumes if necessary
         block_device_info = self._get_instance_volume_block_device_info(
                             context, instance)
-        self.driver.destroy(instance, network_info, block_device_info)
+        self.driver.destroy(instance, network_info, block_device_info,
+                            destroy_disks)
         self._notify_about_instance_usage(
                         context, instance, "live_migration.rollback.dest.end",
                         network_info=network_info)
--- nova-2013.2.2.orig/nova/virt/driver.py
+++ nova-2013.2.2/nova/virt/driver.py
@@ -242,7 +242,7 @@ class ComputeDriver(object):
         raise NotImplementedError()
 
     def destroy(self, instance, network_info, block_device_info=None,
-                destroy_disks=True):
+                destroy_disks=True, migrate_data=None):
         """Destroy (shutdown and delete) the specified instance.
 
         If the instance is not found (for example if networking failed), this
--- nova-2013.2.2.orig/nova/virt/images.py
+++ nova-2013.2.2/nova/virt/images.py
@@ -179,6 +179,26 @@ def convert_image(source, dest, out_form
     utils.execute(*cmd, run_as_root=run_as_root)
 
 
+def direct_fetch(context, image_href, backend_dest, _user_id, _project_id,
+                 max_size=0):
+    """Allow an image backend to fetch directly from the glance backend.
+
+    :backend_dest: the image backend, which must have a direct_fetch
+                   method accepting a list of image locations. This method
+                   should raise exceptions.ImageUnacceptable if the image
+                   cannot be downloaded directly.
+    """
+    # TODO(jdurgin): improve auth handling as noted in fetch()
+    image_service, image_id = glance.get_remote_image_service(context,
+                                                              image_href)
+    locations = image_service.get_locations(context, image_id)
+    image_meta = image_service.show(context, image_id)
+
+    LOG.debug(_('Image locations are: %(locs)s') % {'locs': locations})
+    backend_dest.direct_fetch(image_id, image_meta, locations,
+                              max_size=max_size)
+
+
 def fetch(context, image_href, path, _user_id, _project_id, max_size=0):
     # TODO(vish): Improve context handling and add owner and auth data
     #             when it is added to glance.  Right now there is no
--- nova-2013.2.2.orig/nova/virt/fake.py
+++ nova-2013.2.2/nova/virt/fake.py
@@ -207,7 +207,7 @@ class FakeDriver(driver.ComputeDriver):
         pass
 
     def destroy(self, instance, network_info, block_device_info=None,
-                destroy_disks=True, context=None):
+                destroy_disks=True, context=None, migrate_data=None):
         key = instance['name']
         if key in self.instances:
             del self.instances[key]
--- nova-2013.2.2.orig/nova/virt/libvirt/driver.py
+++ nova-2013.2.2/nova/virt/libvirt/driver.py
@@ -829,10 +829,11 @@ class LibvirtDriver(driver.ComputeDriver
             self._destroy(instance)
 
     def destroy(self, instance, network_info, block_device_info=None,
-                destroy_disks=True, context=None):
+                destroy_disks=True, context=None, migrate_data=None):
         self._destroy(instance)
         self._cleanup(instance, network_info, block_device_info,
-                      destroy_disks, context=context)
+                      destroy_disks, context=context,
+                      migrate_data=migrate_data)
 
     def _undefine_domain(self, instance):
         try:
@@ -866,7 +867,7 @@ class LibvirtDriver(driver.ComputeDriver
                               {'errcode': errcode, 'e': e}, instance=instance)
 
     def _cleanup(self, instance, network_info, block_device_info,
-                 destroy_disks, context=None):
+                 destroy_disks, context=None, migrate_data=None):
         self._undefine_domain(instance)
         self.unplug_vifs(instance, network_info)
         retry = True
@@ -941,9 +942,12 @@ class LibvirtDriver(driver.ComputeDriver
                                  {'vol_id': vol.get('volume_id'), 'exc': exc},
                                  instance=instance)
 
-        if destroy_disks:
+        if destroy_disks or (
+                migrate_data and migrate_data.get('is_shared_block_storage',
+                                                  False)):
             self._delete_instance_files(instance)
 
+        if destroy_disks:
             self._cleanup_lvm(instance)
             #NOTE(haomai): destory volumes if needed
             if CONF.libvirt_images_type == 'rbd':
@@ -952,7 +956,7 @@ class LibvirtDriver(driver.ComputeDriver
     def _cleanup_rbd(self, instance):
         pool = CONF.libvirt_images_rbd_pool
         volumes = libvirt_utils.list_rbd_volumes(pool)
-        pattern = instance['name']
+        pattern = instance['uuid']
 
         def belongs_to_instance(disk):
             return disk.startswith(pattern)
@@ -2380,13 +2384,15 @@ class LibvirtDriver(driver.ComputeDriver
             if size == 0 or suffix == '.rescue':
                 size = None
 
-            image('disk').cache(fetch_func=libvirt_utils.fetch_image,
-                                context=context,
-                                filename=root_fname,
-                                size=size,
-                                image_id=disk_images['image_id'],
-                                user_id=instance['user_id'],
-                                project_id=instance['project_id'])
+            disk_image = image('disk')
+            disk_image.cache(fetch_func=libvirt_utils.fetch_image,
+                             context=context,
+                             filename=root_fname,
+                             size=size,
+                             backend_dest=disk_image,
+                             image_id=disk_images['image_id'],
+                             user_id=instance['user_id'],
+                             project_id=instance['project_id'])
 
         # Lookup the filesystem type if required
         os_type_with_default = disk.get_fs_type_for_os_type(
@@ -3853,16 +3859,15 @@ class LibvirtDriver(driver.ComputeDriver
         # Checking shared storage connectivity
         # if block migration, instances_paths should not be on shared storage.
         source = CONF.host
-        filename = dest_check_data["filename"]
-        block_migration = dest_check_data["block_migration"]
-        is_volume_backed = dest_check_data.get('is_volume_backed', False)
-        has_local_disks = bool(
-                jsonutils.loads(self.get_instance_disk_info(instance['name'])))
 
-        shared = self._check_shared_storage_test_file(filename)
-
-        if block_migration:
-            if shared:
+        dest_check_data.update({'is_shared_block_storage':
+                self._is_shared_block_storage(instance, dest_check_data)})
+        dest_check_data.update({'is_shared_instance_path':
+                self._is_shared_instance_path(dest_check_data)})
+
+        if dest_check_data['block_migration']:
+            if (dest_check_data['is_shared_block_storage'] or
+                    dest_check_data['is_shared_instance_path']):
                 reason = _("Block migration can not be used "
                            "with shared storage.")
                 raise exception.InvalidLocalStorage(reason=reason, path=source)
@@ -3870,11 +3875,11 @@ class LibvirtDriver(driver.ComputeDriver
                                     dest_check_data['disk_available_mb'],
                                     dest_check_data['disk_over_commit'])
 
-        elif not shared and (not is_volume_backed or has_local_disks):
+        elif not (dest_check_data['is_shared_block_storage'] or
+                  dest_check_data['is_shared_instance_path']):
             reason = _("Live migration can not be used "
                        "without shared storage.")
             raise exception.InvalidSharedStorage(reason=reason, path=source)
-        dest_check_data.update({"is_shared_storage": shared})
 
         # NOTE(mikal): include the instance directory name here because it
         # doesn't yet exist on the destination but we want to force that
@@ -3885,6 +3890,21 @@ class LibvirtDriver(driver.ComputeDriver
 
         return dest_check_data
 
+    def _is_shared_block_storage(self, instance, dest_check_data):
+        remote_image_backends = ('rbd',)
+        is_local_image = CONF.libvirt_images_type not in remote_image_backends
+
+        is_volume_backed = dest_check_data.get('is_volume_backed', False)
+        has_local_disks = bool(
+                jsonutils.loads(self.get_instance_disk_info(instance['name'])))
+
+        return (not is_local_image or (
+                    is_volume_backed and not has_local_disks))
+
+    def _is_shared_instance_path(self, dest_check_data):
+        return self._check_shared_storage_test_file(
+                    dest_check_data["filename"])
+
     def _assert_dest_node_has_enough_disk(self, context, instance,
                                              available_mb, disk_over_commit):
         """Checks if destination has enough disk for block migration."""
@@ -4133,17 +4153,21 @@ class LibvirtDriver(driver.ComputeDriver
                            network_info, disk_info, migrate_data=None):
         """Preparation live migration."""
         # Steps for volume backed instance live migration w/o shared storage.
-        is_shared_storage = True
+        is_shared_block_storage = True
+        is_shared_instance_path = True
         is_volume_backed = False
         is_block_migration = True
         instance_relative_path = None
         if migrate_data:
-            is_shared_storage = migrate_data.get('is_shared_storage', True)
+            is_shared_block_storage = migrate_data.get(
+                    'is_shared_block_storage', True)
+            is_shared_instance_path = migrate_data.get(
+                    'is_shared_instance_path', True)
             is_volume_backed = migrate_data.get('is_volume_backed', False)
             is_block_migration = migrate_data.get('block_migration', True)
             instance_relative_path = migrate_data.get('instance_relative_path')
 
-        if not is_shared_storage:
+        if not is_shared_instance_path:
             # NOTE(mikal): this doesn't use libvirt_utils.get_instance_path
             # because we are ensuring that the same instance directory name
             # is used as was at the source
@@ -4157,11 +4181,12 @@ class LibvirtDriver(driver.ComputeDriver
                 raise exception.DestinationDiskExists(path=instance_dir)
             os.mkdir(instance_dir)
 
+        if not is_shared_block_storage:
             # Ensure images and backing files are present.
             self._create_images_and_backing(context, instance, instance_dir,
                                             disk_info)
 
-        if is_volume_backed and not (is_block_migration or is_shared_storage):
+        if not (is_block_migration or is_shared_instance_path):
             # Touch the console.log file, required by libvirt.
             console_file = self._get_console_log_path(instance)
             libvirt_utils.file_open(console_file, 'a').close()
--- nova-2013.2.2.orig/nova/virt/libvirt/imagebackend.py
+++ nova-2013.2.2/nova/virt/libvirt/imagebackend.py
@@ -18,6 +18,7 @@
 import abc
 import contextlib
 import os
+import urllib
 
 from oslo.config import cfg
 
@@ -35,8 +36,10 @@ from nova.virt.libvirt import utils as l
 
 
 try:
+    import rados
     import rbd
 except ImportError:
+    rados = None
     rbd = None
 
 
@@ -69,6 +72,8 @@ CONF = cfg.CONF
 CONF.register_opts(__imagebackend_opts)
 CONF.import_opt('base_dir_name', 'nova.virt.libvirt.imagecache')
 CONF.import_opt('preallocate_images', 'nova.virt.driver')
+CONF.import_opt('rbd_user', 'nova.virt.libvirt.volume')
+CONF.import_opt('rbd_secret_uuid', 'nova.virt.libvirt.volume')
 
 LOG = logging.getLogger(__name__)
 
@@ -193,8 +198,7 @@ class Image(object):
                           (CONF.preallocate_images, self.path))
         return can_fallocate
 
-    @staticmethod
-    def verify_base_size(base, size, base_size=0):
+    def verify_base_size(self, base, size, base_size=0):
         """Check that the base image is not larger than size.
            Since images can't be generally shrunk, enforce this
            constraint taking account of virtual image size.
@@ -213,7 +217,7 @@ class Image(object):
             return
 
         if size and not base_size:
-            base_size = disk.get_disk_size(base)
+            base_size = self.get_disk_size(base)
 
         if size < base_size:
             msg = _('%(base)s virtual size %(base_size)s '
@@ -223,6 +227,9 @@ class Image(object):
                               'size': size})
             raise exception.InstanceTypeDiskTooSmall()
 
+    def get_disk_size(self, name):
+        disk.get_disk_size(name)
+
     def snapshot_create(self):
         raise NotImplementedError()
 
@@ -232,6 +239,14 @@ class Image(object):
     def snapshot_delete(self):
         raise NotImplementedError()
 
+    def direct_fetch(self, image_id, image_meta, image_locations, max_size=0):
+        """Create an image from a direct image location.
+
+        :raises: exception.ImageUnacceptable if it cannot be fetched directly
+        """
+        reason = _('direct_fetch() is not implemented')
+        raise exception.ImageUnacceptable(image_id=image_id, reason=reason)
+
 
 class Raw(Image):
     def __init__(self, instance=None, disk_name=None, path=None,
@@ -435,32 +450,110 @@ class Lvm(Image):
         libvirt_utils.execute(*cmd, run_as_root=True, attempts=3)
 
 
+class RBDVolumeProxy(object):
+    """Context manager for dealing with an existing rbd volume.
+
+    This handles connecting to rados and opening an ioctx automatically, and
+    otherwise acts like a librbd Image object.
+
+    The underlying librados client and ioctx can be accessed as the attributes
+    'client' and 'ioctx'.
+    """
+    def __init__(self, driver, name, pool=None, snapshot=None,
+                 read_only=False):
+        client, ioctx = driver._connect_to_rados(pool)
+        try:
+            self.volume = driver.rbd.Image(ioctx, str(name),
+                                           snapshot=libvirt_utils.ascii_str(
+                                                        snapshot),
+                                           read_only=read_only)
+        except driver.rbd.Error:
+            LOG.exception(_("error opening rbd image %s"), name)
+            driver._disconnect_from_rados(client, ioctx)
+            raise
+        self.driver = driver
+        self.client = client
+        self.ioctx = ioctx
+
+    def __enter__(self):
+        return self
+
+    def __exit__(self, type_, value, traceback):
+        try:
+            self.volume.close()
+        finally:
+            self.driver._disconnect_from_rados(self.client, self.ioctx)
+
+    def __getattr__(self, attrib):
+        return getattr(self.volume, attrib)
+
+
+class RADOSClient(object):
+    """Context manager to simplify error handling for connecting to ceph."""
+    def __init__(self, driver, pool=None):
+        self.driver = driver
+        self.cluster, self.ioctx = driver._connect_to_rados(pool)
+
+    def __enter__(self):
+        return self
+
+    def __exit__(self, type_, value, traceback):
+        self.driver._disconnect_from_rados(self.cluster, self.ioctx)
+
+
 class Rbd(Image):
     def __init__(self, instance=None, disk_name=None, path=None,
                  snapshot_name=None, **kwargs):
         super(Rbd, self).__init__("block", "rbd", is_block_dev=True)
         if path:
             try:
-                self.rbd_name = path.split('/')[1]
+                self.rbd_name = str(path.split('/')[1])
             except IndexError:
                 raise exception.InvalidDevicePath(path=path)
         else:
-            self.rbd_name = '%s_%s' % (instance['name'], disk_name)
+            self.rbd_name = str('%s_%s' % (instance['uuid'], disk_name))
         self.snapshot_name = snapshot_name
         if not CONF.libvirt_images_rbd_pool:
             raise RuntimeError(_('You should specify'
                                  ' libvirt_images_rbd_pool'
                                  ' flag to use rbd images.'))
-        self.pool = CONF.libvirt_images_rbd_pool
-        self.ceph_conf = CONF.libvirt_images_rbd_ceph_conf
+        self.pool = str(CONF.libvirt_images_rbd_pool)
+        self.ceph_conf = libvirt_utils.ascii_str(
+                         CONF.libvirt_images_rbd_ceph_conf)
+        self.rbd_user = libvirt_utils.ascii_str(CONF.rbd_user)
         self.rbd = kwargs.get('rbd', rbd)
+        self.rados = kwargs.get('rados', rados)
+
+        self.path = 'rbd:%s/%s' % (self.pool, self.rbd_name)
+        if self.rbd_user:
+            self.path += ':id=' + self.rbd_user
+        if self.ceph_conf:
+            self.path += ':conf=' + self.ceph_conf
+
+    def _connect_to_rados(self, pool=None):
+        client = self.rados.Rados(rados_id=self.rbd_user,
+                                  conffile=self.ceph_conf)
+        try:
+            client.connect()
+            pool_to_open = str(pool or self.pool)
+            ioctx = client.open_ioctx(pool_to_open)
+            return client, ioctx
+        except self.rados.Error:
+            # shutdown cannot raise an exception
+            client.shutdown()
+            raise
+
+    def _disconnect_from_rados(self, client, ioctx):
+        # closing an ioctx cannot raise an exception
+        ioctx.close()
+        client.shutdown()
 
     def _supports_layering(self):
         return hasattr(self.rbd, 'RBD_FEATURE_LAYERING')
 
     def _ceph_args(self):
         args = []
-        args.extend(['--id', CONF.rbd_user])
+        args.extend(['--id', self.rbd_user])
         args.extend(['--conf', self.ceph_conf])
         return args
 
@@ -482,7 +575,7 @@ class Rbd(Image):
         return hosts, ports
 
     def libvirt_info(self, disk_bus, disk_dev, device_type, cache_mode,
-            extra_specs):
+                     extra_specs, hypervisor_version):
         """Get `LibvirtConfigGuestDisk` filled for this image.
 
         :disk_dev: Disk bus device name
@@ -526,6 +619,17 @@ class Rbd(Image):
 
         return False
 
+    def _resize(self, size):
+        with RBDVolumeProxy(self, self.rbd_name) as vol:
+            vol.resize(int(size))
+
+    def _size(self):
+        return self.get_disk_size(self.rbd_name)
+
+    def get_disk_size(self, name):
+        with RBDVolumeProxy(self, name) as vol:
+            return vol.size()
+
     def create_image(self, prepare_template, base, size, *args, **kwargs):
         if self.rbd is None:
             raise RuntimeError(_('rbd python libraries not found'))
@@ -536,29 +640,104 @@ class Rbd(Image):
             old_format = False
             features = self.rbd.RBD_FEATURE_LAYERING
 
-        if not os.path.exists(base):
+        if not self.check_image_exists():
             prepare_template(target=base, max_size=size, *args, **kwargs)
         else:
             self.verify_base_size(base, size)
 
-        # keep using the command line import instead of librbd since it
-        # detects zeroes to preserve sparseness in the image
-        args = ['--pool', self.pool, base, self.rbd_name]
-        if self._supports_layering():
-            args += ['--new-format']
-        args += self._ceph_args()
-        libvirt_utils.import_rbd_image(*args)
+        # prepare_template may have created the image via direct_fetch()
+        if not self.check_image_exists():
+            # keep using the command line import instead of librbd since it
+            # detects zeroes to preserve sparseness in the image
+            args = ['--pool', self.pool, base, self.rbd_name]
+            if self._supports_layering():
+                args += ['--new-format']
+            args += self._ceph_args()
+            libvirt_utils.import_rbd_image(*args)
+
+        if size and self._size() < size:
+            self._resize(size)
 
     def snapshot_create(self):
         pass
 
     def snapshot_extract(self, target, out_format):
-        snap = 'rbd:%s/%s' % (self.pool, self.rbd_name)
-        images.convert_image(snap, target, out_format)
+        images.convert_image(self.path, target, out_format)
 
     def snapshot_delete(self):
         pass
 
+    def _parse_location(self, location):
+        prefix = 'rbd://'
+        if not location.startswith(prefix):
+            reason = _('Not stored in rbd')
+            raise exception.ImageUnacceptable(image_id=location, reason=reason)
+        pieces = map(urllib.unquote, location[len(prefix):].split('/'))
+        if any(map(lambda p: p == '', pieces)):
+            reason = _('Blank components')
+            raise exception.ImageUnacceptable(image_id=location, reason=reason)
+        if len(pieces) != 4:
+            reason = _('Not an rbd snapshot')
+            raise exception.ImageUnacceptable(image_id=location, reason=reason)
+        return pieces
+
+    def _get_fsid(self):
+        with RADOSClient(self) as client:
+            return client.cluster.get_fsid()
+
+    def _is_cloneable(self, image_location):
+        try:
+            fsid, pool, image, snapshot = self._parse_location(image_location)
+        except exception.ImageUnacceptable as e:
+            LOG.debug(_('not cloneable: %s'), e)
+            return False
+
+        if self._get_fsid() != fsid:
+            reason = _('%s is in a different ceph cluster') % image_location
+            LOG.debug(reason)
+            return False
+
+        # check that we can read the image
+        try:
+            with RBDVolumeProxy(self, image,
+                                pool=pool,
+                                snapshot=snapshot,
+                                read_only=True):
+                return True
+        except self.rbd.Error as e:
+            LOG.debug(_('Unable to open image %(loc)s: %(err)s') %
+                      dict(loc=image_location, err=e))
+            return False
+
+    def _clone(self, pool, image, snapshot):
+        with RADOSClient(self, str(pool)) as src_client:
+            with RADOSClient(self) as dest_client:
+                self.rbd.RBD().clone(src_client.ioctx,
+                                     str(image),
+                                     str(snapshot),
+                                     dest_client.ioctx,
+                                     self.rbd_name,
+                                     features=self.rbd.RBD_FEATURE_LAYERING)
+
+    def direct_fetch(self, image_id, image_meta, image_locations, max_size=0):
+        if self.check_image_exists():
+            return
+        if image_meta.get('disk_format') != 'raw':
+            reason = _('Image is not raw format')
+            raise exception.ImageUnacceptable(image_id=image_id, reason=reason)
+        if not self._supports_layering():
+            reason = _('installed version of librbd does not support cloning')
+            raise exception.ImageUnacceptable(image_id=image_id, reason=reason)
+
+        for location in image_locations:
+            url = location['url']
+            if self._is_cloneable(url):
+                prefix, pool, image, snapshot = self._parse_location(url)
+                return self._clone(pool, image, snapshot)
+
+        reason = _('No image locations are accessible')
+        raise exception.ImageUnacceptable(image_id=image_id, reason=reason)
+
 
 class Backend(object):
     def __init__(self, use_cow):
--- nova-2013.2.2.orig/nova/virt/libvirt/utils.py
+++ nova-2013.2.2/nova/virt/libvirt/utils.py
@@ -45,6 +45,13 @@ CONF.register_opts(libvirt_opts)
 CONF.import_opt('instances_path', 'nova.compute.manager')
 LOG = logging.getLogger(__name__)
 
+try:
+    import rados
+    import rbd
+except ImportError:
+    rados = None
+    rbd = None
+
 
 def execute(*args, **kwargs):
     return utils.execute(*args, **kwargs)
@@ -258,6 +265,33 @@ def create_lvm_image(vg, lv, size, spars
     execute(*cmd, run_as_root=True, attempts=3)
 
 
+def ascii_str(s):
+    """Convert a string to ascii, or return None if the input is None.
+
+    This is useful when a parameter is None by default, or a string. LibRBD
+    only accepts ascii, hence the need for conversion.
+    """
+    if s is None:
+        return s
+    return str(s)
+
+
+def _connect_to_rados(pool=None):
+    ceph_conf = ascii_str(CONF.libvirt_images_rbd_ceph_conf)
+    rbd_user = ascii_str(CONF.rbd_user)
+    client = rados.Rados(rados_id=rbd_user,
+                              conffile=ceph_conf)
+    try:
+        client.connect()
+        pool_to_open = str(pool)
+        ioctx = client.open_ioctx(pool_to_open)
+        return client, ioctx
+    except rados.Error:
+        # shutdown cannot raise an exception
+        client.shutdown()
+        raise
+
+
 def import_rbd_image(*args):
     execute('rbd', 'import', *args)
 
@@ -267,18 +301,20 @@ def list_rbd_volumes(pool):
 
     :param pool: ceph pool name
     """
-    out, err = utils.execute('rbd', '-p', pool, 'ls')
-
-    return [line.strip() for line in out.splitlines()]
+    client, ioctx = _connect_to_rados(pool)
+    rbd_inst = rbd.RBD()
+    return rbd_inst.list(ioctx)
 
 
 def remove_rbd_volumes(pool, *names):
     """Remove one or more rbd volume."""
+    client, ioctx = _connect_to_rados(pool)
+    rbd_inst = rbd.RBD()
     for name in names:
-        rbd_remove = ('rbd', '-p', pool, 'rm', name)
         try:
-            execute(*rbd_remove, attempts=3, run_as_root=True)
-        except processutils.ProcessExecutionError:
+            # Retry if ImageBusy raised
+            rbd_inst.remove(ioctx, name)
+        except rbd.ImageNotFound, rbd.ImageHasSnapshots:
             LOG.warn(_("rbd remove %(name)s in pool %(pool)s failed"),
                      {'name': name, 'pool': pool})
 
@@ -639,8 +675,16 @@ def get_fs_info(path):
             'used': used}
 
 
-def fetch_image(context, target, image_id, user_id, project_id, max_size=0):
+def fetch_image(context, target, image_id, user_id, project_id, max_size=0,
+                backend_dest=None):
     """Grab image."""
+    if backend_dest:
+        try:
+            images.direct_fetch(context, image_id, backend_dest,
+                                user_id, project_id)
+            return
+        except exception.ImageUnacceptable:
+            LOG.debug(_('could not fetch directly, falling back to download'))
     images.fetch_to_raw(context, image_id, target, user_id, project_id,
                         max_size=max_size)
 
